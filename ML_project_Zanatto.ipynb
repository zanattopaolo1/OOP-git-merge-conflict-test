{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQVi5RfETIa/d/lFEOy/a9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zanattopaolo1/OOP-git-merge-conflict-test/blob/master/ML_project_Zanatto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This is (....)"
      ],
      "metadata": {
        "id": "EM5Rhf1W63H4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YooMTdkd1Pw0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "#import math\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions\n",
        "List of functions that will be used inside the code"
      ],
      "metadata": {
        "id": "O3CtcCNJ1glj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def duplicate_check(df, cols):\n",
        "    # Check for duplicates based on the specified columns\n",
        "    duplicate_rows = df[df.duplicated(subset=cols, keep=False)]\n",
        "\n",
        "    return duplicate_rows\n",
        "\n",
        "\"\"\"def filling_UltimoPrelievo(df):\n",
        "    #Having GiornoDaTerapiaPrecedente equal to GiorniTeoriciDaTerapiaPrecedente means that the schedule has been respected. Considering that the cases with we are assuming\n",
        "    df['UltimoPrelievo'] = df['UltimoPrelievo'].where(df['UltimoPrelievo'] == '' & df['GiornoDaTerapiaPrecedente']==df['GiorniTeoriciDaTerapiaPrecedente'], df['DataSomministrazione'] - pd.DateOffset(days=1))\n",
        "\n",
        "    #Removing data where GiornoDaTerapiaPrecedente is NOT equal to GiorniTeoriciDaTerapiaPrecedente\n",
        "    (...)\"\"\"\n",
        "\n",
        "\n",
        "def failed_blood_test(df):\n",
        "    condition1 = df['GiornoDaTerapiaPrecedente'] > df['GiorniTeoriciDaTerapiaPrecedente']  # Define your condition here\n",
        "    condition2 = df['NumeroPrelievi'] > 1\n",
        "    filtered_df = df[condition1 | condition2]\n",
        "    return filtered_df;\n",
        "\n",
        "def del_rows_missing_val (df):\n",
        "    condition = df['UltimoPrelievo'] == ''\n",
        "\n",
        "    rows_to_drop = df[condition].index\n",
        "    return df.drop(rows_to_drop)\n",
        "\n",
        "def data_cleaning(df):\n",
        "    #conditions to drop rows\n",
        "    condition1 = df['UltimoPrelievo'] < df['PrimoPrelievo'] #this information would be caused by a collection error\n",
        "    #condition2 = True   & condition2\n",
        "\n",
        "    rows_to_drop = df[condition1].index\n",
        "    return df.drop(rows_to_drop)\n",
        "\n",
        "\"\"\"def strange_blood_test(df):\n",
        "    condition = df['GiornoDaTerapiaPrecedente'] < df['GiorniTeoriciDaTerapiaPrecedente'] # Define your condition here\n",
        "    filtered_df = df[condition]\n",
        "    return filtered_df;\"\"\"\n",
        "\n",
        "def attrib_removal(df, attrib_list):\n",
        "    for x in attrib_list:\n",
        "        df = df.drop(x, axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def check_value_presence(row):\n",
        "    return {f'{value}': value in row['PrincipiAttivi'] for value in unique_values_set}"
      ],
      "metadata": {
        "id": "vS6vMrqz1c9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading\n",
        "\n",
        "Data import from the csv file into a dictionary, to structure the information in a logical way, and then used to create the Dataframe"
      ],
      "metadata": {
        "id": "LzSgXX301yQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_file_path = 'dataset_farmacia_anonimo(3).csv'\n",
        "\n",
        "keys = ['IDPaziente',\t'genere',\t'classeeta',\t'IDTerapia',\t'CodiceSedeTumore',\t'PrincipiAttivi',\t'PrincipiAttiviDescrizione',\t'NumeroSomministrazione',\t'DataSomministrazioneProgrammata',\t'DataSomministrazione',\t'DataSomministrazioneProgrammataPrecedente',\t'DataSomministrazionePrecedente',\t'GiornoDaTerapiaPrecedente',\t'GiorniTeoriciDaTerapiaPrecedente',\t'PrimoPrelievo',\t'UltimoPrelievo', 'NumeroPrelievi']\n",
        "b_tests = {x: [] for x in keys}\n",
        "\n",
        "with open(dataset_file_path, 'r') as file:\n",
        "    reader = csv.reader(file, delimiter=\";\")\n",
        "    for row_index, row in enumerate(reader):\n",
        "        if row_index > 0:\n",
        "            for idx, cell in enumerate(row):\n",
        "                if \",\" in cell:             #to save the list of active ingredients as array\n",
        "                    b_tests[keys[idx]].append(cell.split(\",\"))\n",
        "                else:\n",
        "                    b_tests[keys[idx]].append(cell)\n",
        "\n",
        "df = pd.DataFrame(b_tests)"
      ],
      "metadata": {
        "id": "w5eWAnjV1xa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualization"
      ],
      "metadata": {
        "id": "8zzlsevY4LmZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.to_string(index=False))\n",
        "\n",
        "#print(tabulate(df, df.columns.tolist(), tablefmt='grid'))"
      ],
      "metadata": {
        "id": "qQ7bx_YR4O_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pre processing\n",
        "The data presents many issues that requires to be fixed\n",
        "\n",
        "1. Duplicated data: Having duplicated data would mean considering multiple times the same information and it will bring a distorsion to reality, so a check is relavant"
      ],
      "metadata": {
        "id": "37b2TF7P4fQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dup_rows = duplicate_check(df, ['IDPaziente', 'IDTerapia', 'NumeroSomministrazione'])\n",
        "print(\"\\n\\nDuplicated rows:\\n\", dup_rows)"
      ],
      "metadata": {
        "id": "UiO1zjKx4e_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Missing values: There are many missing values in the fields \"PrimoPrelievo\" and \"UltimoPrelievo\" and I considered the option to fill \"UltimoPrelievo\". The plan was to fill that field with \"DataSomministrazione\"-1 for each case where the \"GiornoDaTerapiaPrecedente\" is equal to \"GiorniTeoriciDaTerapiaPrecedente\" (so the schedule has been respected) but this option was assuming that all these therapy executions were scheduled on the last 4 days of the week. This is not a feasible assumption when you are interested to compare the results in percentage of blood tests done 1 day beforeand the ones done 3 days before, as this project aims to do. So below the rows where the \"UltimoPrelievo\" (and jointly \"PrimoPrelievo\") are missing will be deleted."
      ],
      "metadata": {
        "id": "enYRRzpM48Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = del_rows_missing_val(df)"
      ],
      "metadata": {
        "id": "ClnurVAP46tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  Casting of data: the import functions extract the data as strings. It makes a problem when the data has to be computed for what they really are."
      ],
      "metadata": {
        "id": "uNueJ6u55mLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['NumeroPrelievi'] = df['NumeroPrelievi'].astype(int)\n",
        "df['DataSomministrazione'] = pd.to_datetime(df['DataSomministrazione'], format='%d/%m/%Y')\n",
        "df['UltimoPrelievo'] = pd.to_datetime(df['UltimoPrelievo'], format='%d/%m/%Y')\n",
        "df['PrimoPrelievo'] = pd.to_datetime(df['PrimoPrelievo'], format='%d/%m/%Y')"
      ],
      "metadata": {
        "id": "M6ENMp6s5l7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Noise: There may occur some error in collection phase of data, so it is important to verify the correctness by means of conditions based on logic of data. These are reported and used in the function \"data_cleaning\"."
      ],
      "metadata": {
        "id": "jK4pLYmK55Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = data_cleaning(df)"
      ],
      "metadata": {
        "id": "_vKxl_a055xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Creation of attribute for the class\n",
        "\n",
        "The goal of this phase is to create a classifier that allow to reach the goal of the algorithm.\n",
        "\n",
        "In order to make it, the first step is to define the classes.\n",
        "In this problem the classes are:\n",
        "\n",
        "\n",
        "*   Ready for the therapy\n",
        "*   Not ready for the therapy\n",
        "\n",
        "To distringuish them it has been used 2 conditions:\n",
        "\n",
        "\n",
        "*   Day of previous therapy is bigger than the scheduled day of previous therapy\n",
        "*   The number of blood test is bigger than 1\n",
        "\n",
        "The rows that reflect at least one of these 2 conditions are considered as \"not ready for the therapy\". Theoretically both of conditions should be valid in the same moment and using only one of them would be sufficient, but in this way it helps to consider the undetected noise of the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "CZkHXQI46MgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "failed_btests = failed_blood_test(df)\n",
        "\n",
        "print(\"\\n\\nThere are {} failed blood tests: {}\\n\".format(len(failed_btests), tabulate(failed_btests, df.columns.tolist(), tablefmt='grid')))\n",
        "print(\"\\n\\nThere are {} failed blood tests ({} % of the total)\\n\".format(len(failed_btests), len(failed_btests)/len(df)*100))"
      ],
      "metadata": {
        "id": "2jDZr0KS6NCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is a strong incidence of failed blood tests and it is important to keep in mind it in front of the solution.\n",
        "\n",
        "Given the rows belonging to the classes, it is created a new column to assign the class."
      ],
      "metadata": {
        "id": "FZ7_DJWP-Dr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_column_values = np.where(df.isin(failed_btests).all(axis=1), False, True)\n",
        "df['Single_btest'] = new_column_values"
      ],
      "metadata": {
        "id": "yq75t-HY-DjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To confirm that the creation has done correctly:\n",
        "#print(tabulate(df, df.columns.tolist(), tablefmt='grid'))\n",
        "#print(\"\\nPercentage of failures of the first blood test: {} %\".format((df['Single_btest']==False).sum() / len(df) * 100))"
      ],
      "metadata": {
        "id": "Bc8oRW0OAKII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the purpose of the project has been fundamental to create also another column to count the day passed between blood test and therapy execution.  "
      ],
      "metadata": {
        "id": "wFXaOwyg_ZZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iff_days = (df['DataSomministrazione'] - df['UltimoPrelievo']).dt.total_seconds().fillna(-1).astype(int)\n",
        "df['Dist_bt_th'] = round(diff_days / (3600 * 24))\n",
        "\n",
        "print(tabulate(df, df.columns.tolist(), tablefmt='grid'))\n",
        "print(\"\\nPercentage of blood tests executed with more than 1 days before therapy execution: {} %\".format((df['Dist_bt_th']>1).sum() / len(df) * 100))\n",
        "print(\"\\nPercentage of blood tests executed 3 days before therapy execution: {} %\".format((df['Dist_bt_th']==3).sum() / len(df) * 100))\n"
      ],
      "metadata": {
        "id": "SJPAAXFj_ZO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  Removal of useless attributes: there are some attributes that are not relevant to be kept for the working operations of the ML algorithm:\n",
        "\n"
      ],
      "metadata": {
        "id": "sstujvh8AQ97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list = ['IDPaziente', 'IDTerapia', 'PrincipiAttiviDescrizione', 'NumeroSomministrazione',\n",
        "        'DataSomministrazioneProgrammata', 'DataSomministrazione', 'DataSomministrazioneProgrammataPrecedente',\n",
        "        'DataSomministrazionePrecedente', 'GiornoDaTerapiaPrecedente',\t'GiorniTeoriciDaTerapiaPrecedente',\n",
        "        'PrimoPrelievo', 'UltimoPrelievo', 'NumeroPrelievi']\n",
        "df = attrib_removal(df, list)\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "0vPeysDZBsv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Management of the attribute \"PrincipiAttivi\" that contains array: this attribute contains an array of active ingredients, but in this format it does not suit with the algorithm operations.\n",
        "\n",
        "\n",
        "> So, to make it usable, it can be created an attribute for each possible value and then setting as True for the rows where that value is present.\n",
        "\n"
      ],
      "metadata": {
        "id": "yi2G9MF94Xet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df, df.apply(check_value_presence, axis=1, result_type='expand')], axis=1)"
      ],
      "metadata": {
        "id": "BXodTPlE4XUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model generation"
      ],
      "metadata": {
        "id": "ZJNfHswvBM3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section will be focused on the concrete creation of the classifier.\n",
        "\n",
        "The first step is the split of the dataset with its classes and, after that, they are divided into:\n",
        "\n",
        "*   Training set (70%)\n",
        "*   Test set (20%)\n",
        "*   Validation set (10%)"
      ],
      "metadata": {
        "id": "dPho0aguBNws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "th_classes = df[['Single_btest', 'Dist_bt_th']]\n",
        "df = df.drop(['Single_btest', 'Dist_bt_th'], axis=1)\n",
        "\n",
        "cutpoint1, cutpoint2 = 0.7, 0,8\n",
        "trainset = df[:int(cutpoint1*len(df))]\n",
        "train_cl = th_classes[:int(cutpoint1*len(df))]\n",
        "valset = df[int(cutpoint1*len(df)):int(cutpoint2*len(df))]\n",
        "val_cl = th_classes[int(cutpoint1*len(df)):int(cutpoint2*len(df))]\n",
        "testset = df[int(cutpoint2*len(df)):]\n",
        "test_cl = th_classes[int(cutpoint2*len(df)):]\n",
        "print('Total: {} splitted in Train: {}, Val: {} and Test: {}'.format(len(df), len(trainset), len(valset), len(testset)))"
      ],
      "metadata": {
        "id": "yfomYAl1BNoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's the turn for the creation of the classifier.\n",
        "\n",
        "There are many possibilities and here we are goinf to try the Support Vector Machines. The parameters used are the following ones (considering this [source](https://vitalflux.com/svm-rbf-kernel-parameters-code-sample/#:~:text=The%20gamma%20parameter%20defines%20how,high%20values%20meaning%20'close'.)):\n",
        "\n",
        "\n",
        "*   kernel = rbf: the dimensionality of data is low\n",
        "*   C = 10: it aims to reduce regularization\n",
        "*   gamma = 0.05: to guarantee sufficient influence of training set\n",
        "\n"
      ],
      "metadata": {
        "id": "USWO5RfTczt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = svm.SVC(gamma=0.1, C=10, kernel='rbf')"
      ],
      "metadata": {
        "id": "wx2l_7qdczkZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}